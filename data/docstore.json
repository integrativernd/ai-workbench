{"docstore/metadata": {"661a96af-57fb-4e8f-9517-8238311f3a95": {"doc_hash": "b8fa8bdb19635ea89186928eaf15fe619a94b28fc689a70818423330ce17e218"}, "8a8d3a34-dac1-41a4-b9ab-ee90c386dc40": {"doc_hash": "7898c0a9f1344425a12fb07676f94ab80b443457f75b6d3c8f6d977e12716feb"}, "1039f598-6f36-4424-b125-213f20fe8d6c": {"doc_hash": "b4487a561a4eba2a721e54e743373d4f92118b69a78a280266f418817ba4ce33"}, "f4356fe5-c43f-4a01-9afd-d8fcf5bc2f46": {"doc_hash": "757a677c691184fe74ef70b63be5ff816f57038e08ddd0d8edf05afcc62cd2c2"}, "0671e9fd-6691-4525-bb87-0a4f1fdc4c25": {"doc_hash": "8f8d9f3ca67ced17a91994a167ed4dbddda31f15e990aac63c03967f12a5a90e"}, "6945afe5-733f-4116-92c0-a6c096c02792": {"doc_hash": "1705488f43bf63df45d9897524def4758814e14ebbbbe8378ecb6fd32d6f3a11"}, "d82ddedc-3bed-4cda-ae73-45ebf4d434ba": {"doc_hash": "0d9b0ce2381f9a215383bdbb7c20db6ba76c74e1225d255865b44d30d2291cef"}, "08e3c918-13e4-4b7d-9270-065d47aa0952": {"doc_hash": "ee12d2169922e45e06b18df246cafad71063f1a1c1480639d031ec524fe186ad"}, "099e13d3-cbfa-4d5d-9f87-bffa035caf3d": {"doc_hash": "42fbd544a5e254765b62b3ff4e90acbc1ec66c63eea659ad76b180c43ae0d7dc"}, "11d1efd9-d86a-4499-86ad-42e0fc3a0f86": {"doc_hash": "2906a438ffcc53e160456d3a187d88636f3c071fd1a11b6edf9ede4eff7f18a2"}, "fcb9c464-7348-4762-a52d-360f6b8ff2c7": {"doc_hash": "c302868ee7079aae5abbfc66d96ed5f09bac64586dc11e92957a2d7974f5cece"}, "58dcf6fe-bf74-4458-be44-319f9b84b36f": {"doc_hash": "9fa264a79fb61ee8b412a4b771e3bbe534007b2a5c3706c64949d052f5570628", "ref_doc_id": "661a96af-57fb-4e8f-9517-8238311f3a95"}, "88b17e9f-956c-4eb1-92bf-e688ed25c258": {"doc_hash": "01fcfd7dbbb6334f228efc36f2ff2c1c1eefc942aea6bca9f4c37a7a210f09ec", "ref_doc_id": "8a8d3a34-dac1-41a4-b9ab-ee90c386dc40"}, "a4b8923d-aff2-4030-9822-b5d83752f55d": {"doc_hash": "1e2d5e9e29c0f7a4b484ebb2001973c68db19dd1a95d82a6f2f7dc9a3a16eb8d", "ref_doc_id": "1039f598-6f36-4424-b125-213f20fe8d6c"}, "ba081ec1-8ed1-4b30-b719-ff5a1aaf5bca": {"doc_hash": "93d5811273a810c603737e6fbce281ec85e9cf1e2132868abff21f7ac511211a", "ref_doc_id": "f4356fe5-c43f-4a01-9afd-d8fcf5bc2f46"}, "97dadca5-260e-4a54-a561-aee4b2ca6af5": {"doc_hash": "ba96c9cfbc6ab9ea428fec6039cb81b4a031bc9aaffffc7b021912839516cf36", "ref_doc_id": "f4356fe5-c43f-4a01-9afd-d8fcf5bc2f46"}, "6dd01734-3b57-4de6-bcae-b7946778a9b8": {"doc_hash": "405ba79cb6c6143a9864005225d01ee6321c9dd36689ec8e16bcbe3d50de6ade", "ref_doc_id": "f4356fe5-c43f-4a01-9afd-d8fcf5bc2f46"}, "40b2f85e-6da8-4382-87e9-e8de4f7e93a2": {"doc_hash": "d67223830873c1f3718d0768c626e1ef8b6235eed8a5e4bf9fd3ae67b7c214e4", "ref_doc_id": "0671e9fd-6691-4525-bb87-0a4f1fdc4c25"}, "99115434-9e9b-46ef-9930-16838ff116eb": {"doc_hash": "61cd8c446f6c1747916789878e5dd9093aef71518cba220ef5ac5672657a1a8b", "ref_doc_id": "6945afe5-733f-4116-92c0-a6c096c02792"}, "1476b07b-878d-4c60-9238-084114b9b4b8": {"doc_hash": "2d9d693b7f62438d4b7a48152c34df6e8d7762505ec636786c790df0c0ebb69a", "ref_doc_id": "d82ddedc-3bed-4cda-ae73-45ebf4d434ba"}, "5abdcdf7-dcbc-44bf-b5c1-ec8933876042": {"doc_hash": "edc8a428087b6dd93a5d3ec8e18d87b893be5ca874f2f55da060d25930e76d69", "ref_doc_id": "08e3c918-13e4-4b7d-9270-065d47aa0952"}, "a5a11c5e-c81e-4878-ae01-bfb820d73b3a": {"doc_hash": "7c63c06f15615d4d004b89d53079064a78996719616d4a0d2ab0a9263a7064c8", "ref_doc_id": "08e3c918-13e4-4b7d-9270-065d47aa0952"}, "f663f254-73d6-4cb3-9fa3-a14c40b83ef4": {"doc_hash": "5ebf72f55e219adfd83c72eb1081908ad7c1e2e51d103ab0d5924b872705f85d", "ref_doc_id": "08e3c918-13e4-4b7d-9270-065d47aa0952"}, "1d52f2a2-ac8a-445f-a0d9-32cb54159664": {"doc_hash": "8252d1556969fed24ca1cf3203c780fd02f7cd977ad38835458fed61f18564f2", "ref_doc_id": "08e3c918-13e4-4b7d-9270-065d47aa0952"}, "58d09407-389f-431a-8771-40e93ad5a18d": {"doc_hash": "b1887d6dcbd12bd06aa0724ced084a5e28bbced9f82afaa07213b6390d6ec0c6", "ref_doc_id": "08e3c918-13e4-4b7d-9270-065d47aa0952"}, "12195939-c697-4e4d-9378-7227ab0bb18c": {"doc_hash": "3f094a6cc8ab9419da13d8049a12bbc1a6940ff861eeb7455c00ad3cfe2882f3", "ref_doc_id": "099e13d3-cbfa-4d5d-9f87-bffa035caf3d"}, "db196e8c-cf3d-47a6-89ad-8fc62cf4eb46": {"doc_hash": "f68bfe802825f0b3c73b989e2ec74c202cb608e1c32761bb563e22bc38693118", "ref_doc_id": "11d1efd9-d86a-4499-86ad-42e0fc3a0f86"}, "a56af8e8-55a2-49c7-bb9b-333e38478a74": {"doc_hash": "72215422d22e2002c05779a628912563f296a879823b63c1290b83a46a9a44ec", "ref_doc_id": "11d1efd9-d86a-4499-86ad-42e0fc3a0f86"}, "f02b6748-107b-4f96-8916-5934ce18129d": {"doc_hash": "cae3fc76d3f21ebbe3c5c03f2649247cfbec1c222dcaef61935d9c496f093a50", "ref_doc_id": "11d1efd9-d86a-4499-86ad-42e0fc3a0f86"}, "9f8889ab-fa1e-44f8-9ce0-7f4fa6ed673d": {"doc_hash": "82e80d8e4b261e9c7ec28addcdbcb9590aeea67064782cad933eef960797a900", "ref_doc_id": "fcb9c464-7348-4762-a52d-360f6b8ff2c7"}}, "docstore/data": {"58dcf6fe-bf74-4458-be44-319f9b84b36f": {"__data__": {"id_": "58dcf6fe-bf74-4458-be44-319f9b84b36f", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/admin.py", "file_name": "admin.py", "file_type": "text/x-python", "file_size": 30, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "661a96af-57fb-4e8f-9517-8238311f3a95", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/admin.py", "file_name": "admin.py", "file_type": "text/x-python", "file_size": 30, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}, "hash": "b8fa8bdb19635ea89186928eaf15fe619a94b28fc689a70818423330ce17e218", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/admin.py\n              \n# Register your models here.", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88b17e9f-956c-4eb1-92bf-e688ed25c258": {"__data__": {"id_": "88b17e9f-956c-4eb1-92bf-e688ed25c258", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/anthropic_integration.py", "file_name": "anthropic_integration.py", "file_type": "text/x-python", "file_size": 2056, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a8d3a34-dac1-41a4-b9ab-ee90c386dc40", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/anthropic_integration.py", "file_name": "anthropic_integration.py", "file_type": "text/x-python", "file_size": 2056, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "7898c0a9f1344425a12fb07676f94ab80b443457f75b6d3c8f6d977e12716feb", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/anthropic_integration.py\n              import os\nfrom anthropic import Anthropic, AsyncAnthropic\nimport asyncio\n\n\nanthropic_client = Anthropic(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n)\n\nasync_anthropic_client = AsyncAnthropic(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n)\n\ndef get_message(system_prompt, tools, messages):\n    return anthropic_client.messages.create(\n        model=\"claude-3-sonnet-20240229\",\n        max_tokens=4000,\n        messages=messages,\n        system=system_prompt,\n        temperature=0,\n        tools=tools,\n    )\n\ndef get_basic_message(system_prompt, messages):\n    return anthropic_client.messages.create(\n        model=\"claude-3-sonnet-20240229\",\n        max_tokens=4000,\n        temperature=0,\n        system=system_prompt,\n        messages=messages,\n    )\n\nasync def send_chunked_message(channel, content):\n    chunk = \"\"\n    for line in content.split('\\n'):\n        if len(chunk) + len(line) + 1 > 1990:  # Leave some room for safety\n            await channel.send(chunk)\n            chunk = line + '\\n'\n        else:\n            chunk += line + '\\n'\n    if chunk:\n        await channel.send(chunk)\n\nasync def stream_to_discord(ai_agent, message):\n    print(\"Streaming to Discord\")\n    buffer = \"\"\n    last_send_time = asyncio.get_event_loop().time()\n\n    async with async_anthropic_client.messages.stream(\n        model=\"claude-3-sonnet-20240229\",\n        max_tokens=1024,\n        system=ai_agent.description,\n        messages=[{\"role\": \"user\", \"content\": message.content}],\n    ) as stream:\n        async for text in stream.text_stream:\n            buffer += text\n            current_time = asyncio.get_event_loop().time()\n            \n            # Send the buffer if it's been 2 seconds or if it's over 1500 characters\n            if current_time - last_send_time > 2 or len(buffer) > 1500:\n                await send_chunked_message(message.channel, buffer)\n                buffer = \"\"\n                last_send_time = current_time\n\n    # Send any remaining text in the buffer\n    if buffer:\n        await send_chunked_message(message.channel, buffer)", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 2197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4b8923d-aff2-4030-9822-b5d83752f55d": {"__data__": {"id_": "a4b8923d-aff2-4030-9822-b5d83752f55d", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/apps.py", "file_name": "apps.py", "file_type": "text/x-python", "file_size": 138, "creation_date": "2024-09-25", "last_modified_date": "2024-09-25"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1039f598-6f36-4424-b125-213f20fe8d6c", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/apps.py", "file_name": "apps.py", "file_type": "text/x-python", "file_size": 138, "creation_date": "2024-09-25", "last_modified_date": "2024-09-25"}, "hash": "b4487a561a4eba2a721e54e743373d4f92118b69a78a280266f418817ba4ce33", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/apps.py\n              from django.apps import AppConfig\n\n\nclass LlmConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'llm'", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba081ec1-8ed1-4b30-b719-ff5a1aaf5bca": {"__data__": {"id_": "ba081ec1-8ed1-4b30-b719-ff5a1aaf5bca", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f4356fe5-c43f-4a01-9afd-d8fcf5bc2f46", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "hash": "757a677c691184fe74ef70b63be5ff816f57038e08ddd0d8edf05afcc62cd2c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97dadca5-260e-4a54-a561-aee4b2ca6af5", "node_type": "1", "metadata": {}, "hash": "907b544b4b0b9538911c99bc060ccac75741cc0684f5fb111ac2c75774634b04", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py\n              import os\nfrom config.settings import SYSTEM_PROMPT, EMBEDDING_MODEL_DIMENSIONS\nfrom llama_index.llms.anthropic import Anthropic\nfrom llama_index.core import Settings\n\nimport re\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n# from llama_index import Document\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.vector_stores.postgres import PGVectorStore\nfrom llama_index.core.node_parser import TokenTextSplitter\nfrom llama_index.core.schema import TextNode\nfrom llama_index.core import (\n  Document,\n  get_response_synthesizer,\n  VectorStoreIndex,\n  SimpleDirectoryReader,\n  load_index_from_storage,\n  StorageContext,\n  set_global_tokenizer,\n)\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.node_parser import TokenTextSplitter\n\nfrom django.core.management.base import BaseCommand, CommandError\nfrom llm.response_types import get_response_type_for_message\nfrom ai_agents.models import CodeRepository, CodeFile\nfrom transformers import AutoTokenizer\n\nEMBED_MODEL = HuggingFaceEmbedding(\n    model_name=\"WhereIsAI/UAE-Large-V1\",\n    embed_batch_size=10 # 24 # open-source embedding model\n)\n\n# set_global_tokenizer(\n#     AutoTokenizer.from_pretrained(f\"mistralai/Mixtral-8x7B-Instruct-v0.1\").encode)\n\ntokenizer = Anthropic().tokenizer\n\nSettings.tokenizer = tokenizer\n\nSettings.llm = Anthropic(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=4000,\n)\n\nSettings.embed_model = EMBED_MODEL\n\n# def save_embeddings(documents: list[Document]) -> None:\n#     for document in documents:\n#         match = re.match(r\"^(\\d{1,3})\", document.text)\n#         if match:\n#             episode_number = int(match.group(1))  # Convert to int for exact matching\n\n#         # Fetch the episode using get() for a single object\n#         episode = CodeRepository.objects.get(episode_number=episode_number)\n        \n#         transcript = Transcript.objects.get(episode=episode)\n\n#         # Generate and save the embedding\n#         embedding = EMBED_MODEL.get_text_embedding(document.text)\n#         transcript.embedding = embedding\n#         transcript.save()", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 2348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97dadca5-260e-4a54-a561-aee4b2ca6af5": {"__data__": {"id_": "97dadca5-260e-4a54-a561-aee4b2ca6af5", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f4356fe5-c43f-4a01-9afd-d8fcf5bc2f46", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "hash": "757a677c691184fe74ef70b63be5ff816f57038e08ddd0d8edf05afcc62cd2c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba081ec1-8ed1-4b30-b719-ff5a1aaf5bca", "node_type": "1", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "hash": "93d5811273a810c603737e6fbce281ec85e9cf1e2132868abff21f7ac511211a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6dd01734-3b57-4de6-bcae-b7946778a9b8", "node_type": "1", "metadata": {}, "hash": "c0c7c5cd16304c0b0167297aa7c366bc65f8bc551c30058026856706802a439d", "class_name": "RelatedNodeInfo"}}, "text": "class Command(BaseCommand):\n    help = 'Test response types'\n\n    def add_arguments(self, parser):\n        parser.add_argument('message', type=str, help='Description of the changes')\n\n    def handle(self, *args, **options):\n        message = options['message'] or \"Describe your source code.\"\n        repository = CodeRepository.objects.get(title=\"ai-workbench\")\n\n        print(repository.title)\n\n        documents = SimpleDirectoryReader(\n            input_dir=\"llm\",\n            recursive=True,\n            filename_as_id=True,\n            required_exts=[\".py\"],\n            exclude=[\n                \".env\",\n                \".venv\",\n                \".certs\",\n                \".pytest_cache\",\n                \".git\", \".idea\",\n                \".vscode\",\n                \"__pycache__\",\n                \"__init__.py\"\n            ],\n        ).load_data()\n\n        # pipeline = IngestionPipeline(transformations=[TokenTextSplitter()])\n\n        # CodeFile.objects.all().delete()\n\n        documents_with_embeddings = []\n\n        file_paths = []\n\n        for doc in documents:\n            # print(doc.metadata['file_path'])\n            text=f\"\"\"\n              file path: {doc.metadata['file_path']}\n              {doc.get_text()}\n              \"\"\"\n            document = Document(\n                metadata=doc.metadata,\n                text=text,\n                embedding=EMBED_MODEL.get_text_embedding(text)\n            )\n            # file_paths.append(doc.metadata['file_path'])\n        #     doc.embedding = EMBED_MODEL.get_text_embedding(doc.get_text())\n            documents_with_embeddings.append(document)\n            \n        #     # embedding = EMBED_MODEL.get_text_embedding(doc.get_text())\n        #     # print(embedding)\n        #     # code_file = CodeFile.objects.create(\n        #     #     repository=repository,\n        #     #     content=doc.get_text(),\n        #     #     file_path=doc.metadata['file_path'],\n        #     #     embedding=embedding,\n        #     # )\n        #     # print(embedding)\n        #     # print(doc.get_text())\n\n        # file_path_text = \"\\n\".join(file_paths)", "mimetype": "text/plain", "start_char_idx": 2355, "end_char_idx": 4461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dd01734-3b57-4de6-bcae-b7946778a9b8": {"__data__": {"id_": "6dd01734-3b57-4de6-bcae-b7946778a9b8", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f4356fe5-c43f-4a01-9afd-d8fcf5bc2f46", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "hash": "757a677c691184fe74ef70b63be5ff816f57038e08ddd0d8edf05afcc62cd2c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97dadca5-260e-4a54-a561-aee4b2ca6af5", "node_type": "1", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "hash": "ba96c9cfbc6ab9ea428fec6039cb81b4a031bc9aaffffc7b021912839516cf36", "class_name": "RelatedNodeInfo"}}, "text": "# nodes = pipeline.run(documents=documents_with_embeddings)\n\n        # vector_store = PGVectorStore.from_params(\n        #     database=\"vector_db\",\n        #     host=\"localhost\",\n        #     password=\"password\",\n        #     port=5432,\n        #     user=\"postgres\",\n        #     table_name=\"ai_workbench_vector_store\",\n        #     embed_dim=1024\n        # )\n\n        persist_dir = \"data\"\n\n        # vector_store.persist(persist_dir)\n\n        # # loaded_vector_store = PGVectorStore.load(persist_dir)\n      \n        # storage_context = StorageContext.from_defaults(persist_dir=\"data\")\n  \n        vector_store_index = VectorStoreIndex.from_documents(\n            documents_with_embeddings,\n            show_progress=True,\n        )\n\n        vector_store_index.storage_context.persist(persist_dir=f\"data\")\n\n        \n        # storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n        # storage_context = StorageContext.from_defaults(\n        #     persist_dir=persist_dir,\n        #     vector_store=vector_store\n        # )\n\n        # index = VectorStoreIndex.from_vector_store(\n        #     vector_store=vector_store,\n        #     embed_model=EMBED_MODEL,\n        # )\n                  \n        # index = load_index_from_storage(storage_context)\n\n        # # index = VectorStoreIndex.from_documents(\n        # #     documents,\n        # #     storage_context=storage_context,\n        # #     show_progress=True\n        # # )\n\n        retriever = VectorIndexRetriever(\n            index=vector_store_index,\n            similarity_top_k=1000,\n            # vector_store_query_mode=\"text_search\",\n            embed_model=EMBED_MODEL,\n        )\n        \n        response_synthesizer = get_response_synthesizer(\n            llm=Anthropic(model=\"claude-3-opus-20240229\"),\n            # response_mode=\"refine\",\n            # streaming=True,\n        )\n\n        query_engine = RetrieverQueryEngine(\n            retriever=retriever,\n            response_synthesizer=response_synthesizer,\n        )\n\n        # query_engine = vector_store_index.as_query_engine(\n        #     llm=Anthropic(model=\"claude-3-opus-20240229\"),\n        #     streaming=True,\n        # )\n\n        streaming_response = query_engine.query(message)\n        \n        # streaming_response.print_response_stream()\n\n        print(streaming_response)", "mimetype": "text/plain", "start_char_idx": 4472, "end_char_idx": 6814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40b2f85e-6da8-4382-87e9-e8de4f7e93a2": {"__data__": {"id_": "40b2f85e-6da8-4382-87e9-e8de4f7e93a2", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/llama_index.py", "file_name": "llama_index.py", "file_type": "text/x-python", "file_size": 3063, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0671e9fd-6691-4525-bb87-0a4f1fdc4c25", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/llama_index.py", "file_name": "llama_index.py", "file_type": "text/x-python", "file_size": 3063, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "hash": "8f8d9f3ca67ced17a91994a167ed4dbddda31f15e990aac63c03967f12a5a90e", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/llama_index.py\n              import os\nfrom llama_index.llms.anthropic import Anthropic\nfrom llama_index.core import Settings\n\ntokenizer = Anthropic().tokenizer\nSettings.tokenizer = tokenizer\n\n\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\n\nfrom django.core.management.base import BaseCommand, CommandError\nfrom llm.response_types import get_response_type_for_message\nfrom llama_index.core import (\n  get_response_synthesizer,\n  VectorStoreIndex,\n  SimpleDirectoryReader,\n  load_index_from_storage,\n  StorageContext\n)\nfrom llama_index.vector_stores.postgres import PGVectorStore\n\n\n\nclass Command(BaseCommand):\n    help = 'Test response types'\n\n    def add_arguments(self, parser):\n        parser.add_argument('message', type=str, help='Description of the changes')\n\n    def handle(self, *args, **options):\n        message = options['message']\n\n        documents = SimpleDirectoryReader(\n            input_dir=\"llm\",\n            exclude=[\n                \".env\",\n                \".venv\",\n                \".certs\",\n                \".pytest_cache\",\n                \".git\", \".idea\",\n                \".vscode\",\n                \"__pycache__\"\n            ],\n        ).load_data()\n\n        for doc in documents:\n            print(doc.metadata['file_path'])\n\n        vector_store = PGVectorStore.from_params(\n            database=\"vector_db\",\n            host=\"localhost\",\n            password=\"password\",\n            port=5432,\n            user=\"postgres\",\n            table_name=\"ai_workbench_vector_store\",\n            embed_dim=1536  # openai embedding dimension\n        )\n\n        persist_dir = \"data\"\n\n        vector_store.persist(persist_dir)\n\n        # loaded_vector_store = PGVectorStore.load(persist_dir)\n      \n        # index = VectorStoreIndex.from_documents(documents)\n        # storage_context = StorageContext.from_defaults(persist_dir=\"data\")\n        # storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n        storage_context = StorageContext.from_defaults(\n            persist_dir=persist_dir,\n            vector_store=vector_store\n        )\n                  \n        index = load_index_from_storage(storage_context)\n\n        # index = VectorStoreIndex.from_documents(\n        #     documents,\n        #     storage_context=storage_context,\n        #     show_progress=True\n        # )\n\n        retriever = VectorIndexRetriever(\n            index=index,\n            similarity_top_k=10,\n        )\n        \n        response_synthesizer = get_response_synthesizer(\n            llm=Anthropic(model=\"claude-3-opus-20240229\"),\n            response_mode=\"simple_summarize\",\n        )\n\n        query_engine = RetrieverQueryEngine(\n            retriever=retriever,\n            response_synthesizer=response_synthesizer,\n        )\n\n        # index.storage_context.persist(persist_dir=f\"data\")\n\n        # query_engine = index.as_query_engine(\n        #     llm=Anthropic(model=\"claude-3-opus-20240229\")\n        # )\n\n        response = query_engine.query(message)\n\n        print(response)", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 3214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99115434-9e9b-46ef-9930-16838ff116eb": {"__data__": {"id_": "99115434-9e9b-46ef-9930-16838ff116eb", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/response_types.py", "file_name": "response_types.py", "file_type": "text/x-python", "file_size": 659, "creation_date": "2024-09-30", "last_modified_date": "2024-09-30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6945afe5-733f-4116-92c0-a6c096c02792", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/response_types.py", "file_name": "response_types.py", "file_type": "text/x-python", "file_size": 659, "creation_date": "2024-09-30", "last_modified_date": "2024-09-30"}, "hash": "1705488f43bf63df45d9897524def4758814e14ebbbbe8378ecb6fd32d6f3a11", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/response_types.py\n              from django.core.management.base import BaseCommand, CommandError\nfrom llm.response_types import get_response_type_for_message\n\nclass Command(BaseCommand):\n    help = 'Test response types'\n\n    def add_arguments(self, parser):\n        parser.add_argument('message', type=str, help='Description of the changes')\n\n    def handle(self, *args, **options):\n        message = options['message']\n        try:\n            self.stdout.write(self.style.SUCCESS('Checking response type'))\n            result = get_response_type_for_message(message)\n            print(result)\n        except Exception as e:\n            raise CommandError('An error occurred: %s' % str(e))", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1476b07b-878d-4c60-9238-084114b9b4b8": {"__data__": {"id_": "1476b07b-878d-4c60-9238-084114b9b4b8", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/models.py", "file_name": "models.py", "file_type": "text/x-python", "file_size": 28, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d82ddedc-3bed-4cda-ae73-45ebf4d434ba", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/models.py", "file_name": "models.py", "file_type": "text/x-python", "file_size": 28, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}, "hash": "0d9b0ce2381f9a215383bdbb7c20db6ba76c74e1225d255865b44d30d2291cef", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/models.py\n              \n# Create your models here.", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5abdcdf7-dcbc-44bf-b5c1-ec8933876042": {"__data__": {"id_": "5abdcdf7-dcbc-44bf-b5c1-ec8933876042", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08e3c918-13e4-4b7d-9270-065d47aa0952", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "ee12d2169922e45e06b18df246cafad71063f1a1c1480639d031ec524fe186ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5a11c5e-c81e-4878-ae01-bfb820d73b3a", "node_type": "1", "metadata": {}, "hash": "1f386fdf0294180020fa9212e2eb2d585781096a89dc922d9d2d7af55c14a7dc", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py\n              from abc import ABC, abstractmethod\nfrom asgiref.sync import sync_to_async\nfrom channels.models import Message\nfrom datetime import datetime\nfrom rq.job import Job\nfrom typing import Dict, List, Any\nimport django_rq\nimport inspect\nimport pytz\n\nfrom config.settings import PRODUCTION, BASE_DIR\nfrom llm.anthropic_integration import get_message, get_basic_message\nfrom tools.config import TOOL_DEFINITIONS\nfrom tools.search import get_search_data\nfrom tools.browse import get_web_page_content\nfrom tools.google.docs import append_text, read_document\nfrom tools.github.pull_requests import open_pull_request\nfrom tools.github.issues import create_github_issue, read_github_issue\nfrom temporal_app.run_workflow import run_workflow, get_temporal_client\n\n\n\ndef get_current_time():\n    est = pytz.timezone('US/Eastern')\n    est_time = datetime.now(est)\n    return est_time.strftime('%B %d, %Y, %I:%M %p')\n\n\ndef get_runtime_environment():\n    return 'PRODUCTION' if PRODUCTION else 'DEVELOPMENT'\n\n\ndef approximate_token_count(text):\n    return int(len(text.split()) / 0.75)\n\n\ndef summarize_content(system_prompt, content):\n    message = get_basic_message(\n        system_prompt,\n        [{\"role\": \"user\", \"content\": content}]\n    )\n    return message.content[0].text\n\n\ndef create_system_prompt(base_prompt, additional_context=\"\"):\n    return f\"\"\"\n    BASE_PROMPT\n    {base_prompt}\n    BACKGROUND_PROCESS\n    You are at the point in the response cycle where you need to generate the content\n    to address the user's request. You are in a running background process and \n    you can respond in a single response in the slack thread with a maximum of 2000 characters.\n    {additional_context}\n    \"\"\"\n\n# AI: TOOL RESPONDERS\n\n# These classes are used to define the tools that the AI can use to respond to user requests.\n# Each tool is a class that inherits from BaseTool and implements the execute method.\nclass BaseTool(ABC):\n    def __init__(self, input_keys: List[str] = None, immediate: bool = False):\n        self.input_keys = input_keys or []\n        self.immediate = immediate\n\n    @abstractmethod\n    def execute(self, request_data: Dict[str, Any]) -> str:\n        pass\nclass GetTimeTool(BaseTool):\n    def __init__(self):\n        super().__init__()\n        self.immediate = True\n\n    def execute(self, _):\n        return f\"current time: {get_current_time()}\"\n\nclass GetRuntimeEnvironmentTool(BaseTool):\n    def __init__(self):\n        super().__init__()\n        self.immediate = True\n\n    def execute(self, request_data):\n        return get_runtime_environment()\n\nclass GetSearchResultsTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"query\"])\n\n    def execute(self, request_data):\n        request_data['content'] = summarize_content(\n            f\"\"\"\n            Respond to the user's query {request_data['query']} concisely and accurately.\n            Based on the search results provided in the message. Limit your response\n            to a maximum of 2000 characters.", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 3114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5a11c5e-c81e-4878-ae01-bfb820d73b3a": {"__data__": {"id_": "a5a11c5e-c81e-4878-ae01-bfb820d73b3a", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08e3c918-13e4-4b7d-9270-065d47aa0952", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "ee12d2169922e45e06b18df246cafad71063f1a1c1480639d031ec524fe186ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5abdcdf7-dcbc-44bf-b5c1-ec8933876042", "node_type": "1", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "edc8a428087b6dd93a5d3ec8e18d87b893be5ca874f2f55da060d25930e76d69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f663f254-73d6-4cb3-9fa3-a14c40b83ef4", "node_type": "1", "metadata": {}, "hash": "c2d9076c2ec0938c1d2efc29f8c1fcbef8b336d8000d4b3fe49a183439dd7806", "class_name": "RelatedNodeInfo"}}, "text": "Based on the search results provided in the message. Limit your response\n            to a maximum of 2000 characters.\n            \"\"\",\n            get_search_data(request_data['query'])\n        )\n        return request_data\n\nclass GetWebPageSummaryTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"url\"])\n\n    def execute(self, request_data):\n        request_data['content'] = summarize_content(\n            request_data['ai_agent_system_prompt'],\n            get_web_page_content(request_data['url'])\n        )\n        return request_data\n\nclass ReadGoogleDocumentTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"google_doc_id\"])\n\n    def execute(self, request_data):\n        prompt = create_system_prompt(request_data['ai_agent_system_prompt'])\n        message = get_basic_message(\n            prompt,\n            [{ \"role\": \"user\", \"content\": read_document(request_data['google_doc_id']) }]\n        )\n        request_data['content'] = message.content[0].text\n        return request_data\n\nclass ReadSystemArchitectureTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"query\"])\n        # self.immediate = True\n\n    def execute(self, request_data):\n        print('Reading system architecture')\n        print(f\"Considering user's request {request_data['query']}\")\n        with open(f'{BASE_DIR}/system_architecture.txt', 'r') as file:\n            project_overview = file.read()\n        message = get_basic_message(\n            request_data['query'],\n            [{ \"role\": \"user\", \"content\": project_overview }]\n        )\n        tool_sequence = request_data['tool_sequence']\n        new_tool_sequence = [t for t in tool_sequence if t.name != \"read_system_architecture\"]\n        request_data['tool_sequence'] = new_tool_sequence\n        if len(request_data['tool_sequence']) > 0:\n            request_data['content'] = message.content[0].text\n        else:\n            request_data['content'] = message.content[0].text[:2000]\n        return request_data\n\nclass UpdateGoogleDocumentTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"google_doc_id\", \"content\"])\n\n    def execute(self, request_data):\n        document_content_object = read_document(request_data['google_doc_id'])\n        print(f\"Document content object: {document_content_object}\")\n        system_prompt = f\"\"\"\n        AI AGENT SYSTEM PROMPT: {request_data['ai_agent_system_prompt']}\n        CURRENT_DOCUMENT_CONTENT: {document_content_object}\n        Respond to the user's request to update the Google document with just the \n        new content and no other information. Limit your response to a maximum of 2000 characters.\n        \"\"\"\n        message = get_basic_message(system_prompt, [\n            {\n                \"role\": \"user\",\n                \"content\": request_data['content']\n            }\n        ])\n        response_text = message.content[0].text\n        try:\n            print(\"Updating document with text: google_doc_id \")\n            append_text(request_data['google_doc_id'], response_text)\n        except Exception as e:\n            print(f\"Error updating document: {e}\")\n            request_data['content'] = \"Error updating document.\"\n\n        request_data['content'] = \"Document updated.\"\n        return request_data\n\nclass OpenPullRequestTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"title\", \"description\"])\n\n    def execute(self, request_data):\n        print(f\"Opening pull request with title: {request_data['title']}\")\n        print(f\"Opening pull request with description: {request_data['description']}\")\n        try:\n            request_data['content'] =  open_pull_request(request_data)\n        except Exception as e:\n            print(f\"Error opening pull request: {e}\")\n            request_data['content'] = \"Error opening pull request.\"", "mimetype": "text/plain", "start_char_idx": 2997, "end_char_idx": 6814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f663f254-73d6-4cb3-9fa3-a14c40b83ef4": {"__data__": {"id_": "f663f254-73d6-4cb3-9fa3-a14c40b83ef4", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08e3c918-13e4-4b7d-9270-065d47aa0952", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "ee12d2169922e45e06b18df246cafad71063f1a1c1480639d031ec524fe186ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5a11c5e-c81e-4878-ae01-bfb820d73b3a", "node_type": "1", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "7c63c06f15615d4d004b89d53079064a78996719616d4a0d2ab0a9263a7064c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d52f2a2-ac8a-445f-a0d9-32cb54159664", "node_type": "1", "metadata": {}, "hash": "88d8f17281690d27e3632f734dc3e70c094059640007e8e20da18496b8dba199", "class_name": "RelatedNodeInfo"}}, "text": "request_data['content'] = \"Document updated.\"\n        return request_data\n\nclass OpenPullRequestTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"title\", \"description\"])\n\n    def execute(self, request_data):\n        print(f\"Opening pull request with title: {request_data['title']}\")\n        print(f\"Opening pull request with description: {request_data['description']}\")\n        try:\n            request_data['content'] =  open_pull_request(request_data)\n        except Exception as e:\n            print(f\"Error opening pull request: {e}\")\n            request_data['content'] = \"Error opening pull request.\"\n        return request_data\n    \nclass GetBackgroundJobsTool(BaseTool):\n    def __init__(self):\n        super().__init__()\n        self.immediate = True\n\n    def execute(self, request_data):\n        print(\"Getting background jobs\")\n        message_queue = django_rq.get_queue(\"default\")\n        job_ids = message_queue.started_job_registry.get_job_ids()\n        if len(job_ids) == 0:\n            return \"Not working on anything at the moment.\"\n        else:\n            summary = f\"Task count: {len(job_ids)}\\n\"\n            jobs = Job.fetch_many(job_ids, connection=message_queue.connection)\n            for job in jobs:\n                summary += job.args[0]['content']\n            return summary\n\nclass CreateGithubIssueTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"title\", \"description\"])\n\n    def execute(self, request_data):\n        request_data['content'] = create_github_issue(request_data)\n        return request_data\n    \nclass AskClarifyingQuestionTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"request\"])\n\n    def execute(self, request_data):\n        message = get_basic_message(\n            f\"\"\"\n            {request_data['ai_agent_system_prompt']}\n            Acknowledge the user's request to address the issue and ask one or more\n            clarifying questions to better understand the issue. Limit your response\n            to a maximum of 2000 characters.\n            \"\"\",\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": request_data['request'],\n                }\n            ]\n        )\n        request_data['content'] = message.content[0].text\n        return request_data\n\nclass AnalyzeGithubIssueTool(BaseTool):\n    def __init__(self):\n        super().__init__([\"issue_number\", \"issue_url\", \"description\"])\n\n    def execute(self, request_data):\n        title, body = read_github_issue(request_data)\n        prompt = create_system_prompt(\n            request_data['ai_agent_system_prompt'],\n            \"\"\"\n            Acknowledged the user's request to address the GitHub issue.\n            Respond with your current understanding of the issue.\n            \"\"\"\n        )\n        message = get_basic_message(\n            prompt,\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                    User request: {request_data['description']}\n                    Title: {title}\n                    Description: {body}\n                    \"\"\"\n                }\n            ]\n        )\n        request_data['content'] = message.content[0].text\n        return request_data\n\n# AI ADD CLASSES HERE\n\nclass ToolRegistry:\n    \"\"\"\n    The ToolRegistry class is used to register and manage the tools that the AI\n    can use to respond to user requests. It automatically registers all classes\n    that inherit from BaseTool in the current module.\n    \"\"\"", "mimetype": "text/plain", "start_char_idx": 6191, "end_char_idx": 9725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d52f2a2-ac8a-445f-a0d9-32cb54159664": {"__data__": {"id_": "1d52f2a2-ac8a-445f-a0d9-32cb54159664", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08e3c918-13e4-4b7d-9270-065d47aa0952", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "ee12d2169922e45e06b18df246cafad71063f1a1c1480639d031ec524fe186ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f663f254-73d6-4cb3-9fa3-a14c40b83ef4", "node_type": "1", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "5ebf72f55e219adfd83c72eb1081908ad7c1e2e51d103ab0d5924b872705f85d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58d09407-389f-431a-8771-40e93ad5a18d", "node_type": "1", "metadata": {}, "hash": "2e5df9e968f5f6e9b5b8f8bdd28271829f297584b15d6bad384821e0dde9737a", "class_name": "RelatedNodeInfo"}}, "text": "Respond with your current understanding of the issue.\n            \"\"\"\n        )\n        message = get_basic_message(\n            prompt,\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                    User request: {request_data['description']}\n                    Title: {title}\n                    Description: {body}\n                    \"\"\"\n                }\n            ]\n        )\n        request_data['content'] = message.content[0].text\n        return request_data\n\n# AI ADD CLASSES HERE\n\nclass ToolRegistry:\n    \"\"\"\n    The ToolRegistry class is used to register and manage the tools that the AI\n    can use to respond to user requests. It automatically registers all classes\n    that inherit from BaseTool in the current module.\n    \"\"\"\n    \n    def __init__(self):\n        self.tools: Dict[str, BaseTool] = {}\n\n    def register(self, name: str, tool: BaseTool):\n        self.tools[name] = tool\n\n    def get(self, name: str) -> BaseTool:\n        return self.tools.get(name)\n\n    def auto_register_tools(self):\n        # Get all classes in the current module that inherit from BaseTool\n        tool_classes = {\n            name: cls for name, cls in globals().items()\n                if inspect.isclass(cls) and issubclass(cls, BaseTool) and cls != BaseTool\n        }        \n        for tool_def in TOOL_DEFINITIONS:\n            tool_name = tool_def['name']\n            class_name = ''.join(word.capitalize() for word in tool_name.split('_')) + 'Tool'\n            if class_name in tool_classes:\n                tool_class = tool_classes[class_name]\n                tool_instance = tool_class()\n                self.register(tool_name, tool_instance)\n            else:\n                print(f\"Warning: No matching class found for tool '{tool_name}'\")\n\n    def handle_tool_use(self, ai_agent, request_data):\n        tool_call = request_data['tool']\n        tool = self.get(tool_call.name)\n        \n        if not tool:\n            return f\"Unknown tool: {tool_call.name}\"\n        # Collect the input data for the tool\n        for key in tool.input_keys:\n            if key in tool_call.input:\n                request_data[key] = tool_call.input[key]\n        print(f\"Executing tool: {tool_call.name}\")\n        if tool.immediate:\n            print(\"Executing tool immediately\")\n            return tool.execute(request_data)\n\n        # enqueue_result = django_rq.enqueue(tool.execute, request_data)\n\n        # if isinstance(enqueue_result, str):\n        #     return enqueue_result\n        # elif hasattr(enqueue_result, 'id'):\n        #     ai_agent.add_job(enqueue_result.id)\n        #     ai_agent.save()\n        #     response_text = f\"\"\"\n        #     State that you are using the {tool_call.name} in a concise and natural way.\n        #     \"\"\"\n        #     response = ai_agent.respond_to_user(response_text)\n        #     return f\"{response}\\nJOB ID: {enqueue_result.id}\"\n        # else:\n        return \"Processing started\"\n\n# Initialize the tool registry\ntool_registry = ToolRegistry()\n# tool_registry.auto_register_tools()\n\n# AI: RESPONSE HANDLING\n\ndef persist_message(channel, request_data):\n    try:\n        Message.objects.create(\n            channel=channel,\n            author=request_data['author'],\n            content=request_data['content'],\n        )\n    except Exception as e:\n        print(f\"Error saving message: {e}\")", "mimetype": "text/plain", "start_char_idx": 8920, "end_char_idx": 12325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58d09407-389f-431a-8771-40e93ad5a18d": {"__data__": {"id_": "58d09407-389f-431a-8771-40e93ad5a18d", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08e3c918-13e4-4b7d-9270-065d47aa0952", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "ee12d2169922e45e06b18df246cafad71063f1a1c1480639d031ec524fe186ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d52f2a2-ac8a-445f-a0d9-32cb54159664", "node_type": "1", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "8252d1556969fed24ca1cf3203c780fd02f7cd977ad38835458fed61f18564f2", "class_name": "RelatedNodeInfo"}}, "text": "#     \"\"\"\n        #     response = ai_agent.respond_to_user(response_text)\n        #     return f\"{response}\\nJOB ID: {enqueue_result.id}\"\n        # else:\n        return \"Processing started\"\n\n# Initialize the tool registry\ntool_registry = ToolRegistry()\n# tool_registry.auto_register_tools()\n\n# AI: RESPONSE HANDLING\n\ndef persist_message(channel, request_data):\n    try:\n        Message.objects.create(\n            channel=channel,\n            author=request_data['author'],\n            content=request_data['content'],\n        )\n    except Exception as e:\n        print(f\"Error saving message: {e}\")\n\n\n@sync_to_async\ndef save_ai_agent_workflow(ai_agent, workflow):\n    if not ai_agent or not workflow:\n        return\n\n    ai_agent.add_job(workflow.id)\n    return workflow.id\n\nasync def handle_tool_contents(ai_agent, user_message, tool_contents):\n    if len(tool_contents) == 0:\n        return\n\n    try:\n        return tool_registry.handle_tool_use(ai_agent, {\n            \"ai_agent_system_prompt\": ai_agent.description,\n            \"ai_agent_id\": ai_agent.id,\n            \"channel_id\": user_message.channel.id,\n            \"content\": user_message.content,\n            \"tool\": tool_contents[0],\n            \"tool_sequence\": tool_contents[1:],\n        })\n    except Exception as e:\n        return f\"Error processing tool: {e}\"\n\n\nasync def respond(ai_agent, user_message):\n    # Perform tool actions\n    try:\n        workflow = await run_workflow(\n            ai_agent,\n            user_message\n        )\n        if workflow:\n            await save_ai_agent_workflow(ai_agent, workflow)\n        return \"Workflow started\"\n    except:\n        return \"Workflow failed to start\"", "mimetype": "text/plain", "start_char_idx": 11725, "end_char_idx": 13398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12195939-c697-4e4d-9378-7227ab0bb18c": {"__data__": {"id_": "12195939-c697-4e4d-9378-7227ab0bb18c", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/response_types.py", "file_name": "response_types.py", "file_type": "text/x-python", "file_size": 1350, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "099e13d3-cbfa-4d5d-9f87-bffa035caf3d", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/response_types.py", "file_name": "response_types.py", "file_type": "text/x-python", "file_size": 1350, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}, "hash": "42fbd544a5e254765b62b3ff4e90acbc1ec66c63eea659ad76b180c43ae0d7dc", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/response_types.py\n              import os\nimport instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\nfrom enum import Enum\n\n\nclass ResponseType(Enum):\n    MESSAGE = \"message\"\n    TOOL = \"tool\"\n\n\nclass ResponseModel(BaseModel):\n    type: ResponseType\n\n\ndef get_response_type_for_message(ai_agent, message_content):\n    client = instructor.from_anthropic(Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\")))\n    # tool_names = \",\".join([tool[\"name\"] for tool in TOOL_DEFINITIONS])\n    response = client.messages.create(\n        model=\"claude-3-opus-20240229\",\n        system=\"\"\"\n        You are an AI Agent designed to do one thing: determine if an AI Agent should\n        respond to the user using a call to the base large language model (LLM) or if\n        it need to call a tool or external service to be able to respond to the user.\n        Based on the content provided from user consider all factors and determine\n        if the base LLM should be used or a tool should be used. If the base LLM can be\n        used return the response type as 'message'. If a tool should be used return the\n        response type as 'tool'.\n        \"\"\",\n        max_tokens=1024,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": message_content,\n            }\n        ],\n        response_model=ResponseModel,\n    )\n    return response", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 1484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db196e8c-cf3d-47a6-89ad-8fc62cf4eb46": {"__data__": {"id_": "db196e8c-cf3d-47a6-89ad-8fc62cf4eb46", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11d1efd9-d86a-4499-86ad-42e0fc3a0f86", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "2906a438ffcc53e160456d3a187d88636f3c071fd1a11b6edf9ede4eff7f18a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a56af8e8-55a2-49c7-bb9b-333e38478a74", "node_type": "1", "metadata": {}, "hash": "9aab2295086db4686dccbf2ae0774b3595d2f45283a7759004d398ff0d0aeef9", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py\n              import os\nfrom django.test import TestCase\nfrom llm.anthropic_integration import get_message, get_basic_message\nimport json\nfrom config.settings import BASE_DIR, SYSTEM_PROMPT\nfrom tools.config import TOOL_DEFINITIONS\nimport pickle\n\nsearch_file_path = 'llm/tests/fixtures/serper_result.json'\nwith open(os.path.join(BASE_DIR, search_file_path), 'r') as file:\n    search_results_fixture = json.load(file)\n\nRECORD_FIXTURES = False\nUSE_FIXTURES = True\n\nclass AnthropicIntegrationTest(TestCase):\n    def setUp(self):\n        self.fixtures_dir = os.path.join(BASE_DIR, 'llm/tests/fixtures')\n        os.makedirs(self.fixtures_dir, exist_ok=True)\n\n    def tearDown(self):\n        pass\n\n    def get_or_record_basic_message(self, system_prompt, messages, fixture_name, record_fixtures=RECORD_FIXTURES):\n        fixture_path = os.path.join(self.fixtures_dir, f\"{fixture_name}.pickle\")\n\n        if record_fixtures:\n            message = get_basic_message(system_prompt, messages)\n            with open(fixture_path, 'wb') as f:\n                pickle.dump(message, f)\n            return message\n        else:\n            print('Using fixture')\n            if os.path.exists(fixture_path):\n                with open(fixture_path, 'rb') as f:\n                    return pickle.load(f)\n            else:\n                raise FileNotFoundError(f\"Fixture {fixture_path} not found. Run tests with RECORD_FIXTURES=true to generate it.\")\n\n    def get_or_record_tool_message(self, system_prompt, messages, fixture_name, record_fixtures=RECORD_FIXTURES):\n        fixture_path = os.path.join(self.fixtures_dir, f\"{fixture_name}.pickle\")\n\n        if record_fixtures:\n            message = get_message(system_prompt, TOOL_DEFINITIONS, messages)\n            with open(fixture_path, 'wb') as f:\n                pickle.dump(message, f)\n            return message\n        else:\n            print('Using fixture')\n            if os.path.exists(fixture_path):\n                with open(fixture_path, 'rb') as f:\n                    return pickle.load(f)\n            else:\n                raise FileNotFoundError(f\"Fixture {fixture_path} not found. Run tests with RECORD_FIXTURES=true to generate it.\")\n    \n    # def test_basic_message(self):\n    #     message = self.get_or_record_basic_message(\n    #         \"If you receive ping respond with only Pong and nothing else\",\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": \"Ping\",\n    #             }\n    #         ],\n    #         \"basic_message_test\",\n    #         record_fixtures=False,\n    #     )\n    #     self.assertEqual(message.content[0].text, \"Pong\")\n\n    # def test_performance(self):\n    #     message = self.get_or_record_basic_message(\n    #         \"\"\"\n    #             Summarize the provided JSON search result data to answer the user's question in a single word.\n    #             Return a strict single word response for the purposes of being used in a test case.\n    #             Question: What was John Adam's secret name?\n    #             Answer: Lando\n    #         \"\"\",\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": f\"\"\"\n    #                 Question: What was George Washington's secret name?", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 3413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a56af8e8-55a2-49c7-bb9b-333e38478a74": {"__data__": {"id_": "a56af8e8-55a2-49c7-bb9b-333e38478a74", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11d1efd9-d86a-4499-86ad-42e0fc3a0f86", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "2906a438ffcc53e160456d3a187d88636f3c071fd1a11b6edf9ede4eff7f18a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db196e8c-cf3d-47a6-89ad-8fc62cf4eb46", "node_type": "1", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "f68bfe802825f0b3c73b989e2ec74c202cb608e1c32761bb563e22bc38693118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f02b6748-107b-4f96-8916-5934ce18129d", "node_type": "1", "metadata": {}, "hash": "2e54400ab078182a3667a12858d1639cebfc9cd7c13c1b4d0167a37a2e44bb87", "class_name": "RelatedNodeInfo"}}, "text": "#             Return a strict single word response for the purposes of being used in a test case.\n    #             Question: What was John Adam's secret name?\n    #             Answer: Lando\n    #         \"\"\",\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": f\"\"\"\n    #                 Question: What was George Washington's secret name?\n    #                 SEARCH JSON: {search_results_fixture}\n    #                 \"\"\",\n    #             }\n    #         ],\n    #         \"performance_test\",\n    #         record_fixtures=False,\n    #     )\n    #     self.assertEqual(message.content[0].text, \"Waldo\")\n\n    def test_get_time_tool_message(self):\n        message = self.get_or_record_tool_message(\n            SYSTEM_PROMPT,\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Please help me plan a business based on your capabilities\",\n                }\n            ],\n            \"simple_tool_message\",\n            record_fixtures=True,\n        ) \n\n        # print(message.content[0].text)\n        self.assertEqual(len(message.content), 2)\n        self.assertEqual(message.content[1].name, 'conduct_swot_analysis')\n    \n    # def test_get_time_tool_message(self):\n    #     message = self.get_or_record_tool_message(\n    #         SYSTEM_PROMPT,\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": \"What time is it?\",\n    #             }\n    #         ],\n    #         \"simple_tool_message\",\n    #         record_fixtures=True,\n    #     )\n    #     self.assertEqual(message.content[0].name, 'get_current_time')\n\n    # def test_get_runtime_environment_tool_message(self):\n    #     message = self.get_or_record_tool_message(\n    #         SYSTEM_PROMPT,\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": \"What runtime environment are you in?\",\n    #             }\n    #         ],\n    #         \"runtime_env_query\",\n    #         record_fixtures=False,\n    #     )\n    #     self.assertEqual(message.content[0].name, 'get_runtime_environment')\n\n    # def test_multiple_tool_calls(self):\n    #     message = self.get_or_record_tool_message(\n    #         SYSTEM_PROMPT,\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": \"What time is it? What runtime environment are you in?\",\n    #             }\n    #         ],\n    #         \"multiple_tool_calls\",\n    #         record_fixtures=False,\n    #     )\n    #     self.assertEqual(len(message.content), 2)\n    #     self.assertEqual(message.content[0].name, 'get_time')\n    #     self.assertEqual(message.content[1].name, 'get_runtime_environment')\n    \n    # def test_read_systems_architecture(self):\n    #     message = self.get_or_record_tool_message(\n    #         SYSTEM_PROMPT,\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": \"Tell me about your technical systems.\",\n    #             }\n    #         ],\n    #         \"read_systems_architecture\",\n    #         record_fixtures=False,\n    #     )\n    #     self.assertEqual(message.content[0].name, 'read_system_architecture')\n    \n    # def test_read_systems_architecture_and_update_google_document(self):\n    #     message = self.get_or_record_tool_message(\n    #         SYSTEM_PROMPT,\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": \"Update the google document with details about your implementation.", "mimetype": "text/plain", "start_char_idx": 3017, "end_char_idx": 6619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f02b6748-107b-4f96-8916-5934ce18129d": {"__data__": {"id_": "f02b6748-107b-4f96-8916-5934ce18129d", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11d1efd9-d86a-4499-86ad-42e0fc3a0f86", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "2906a438ffcc53e160456d3a187d88636f3c071fd1a11b6edf9ede4eff7f18a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a56af8e8-55a2-49c7-bb9b-333e38478a74", "node_type": "1", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}, "hash": "72215422d22e2002c05779a628912563f296a879823b63c1290b83a46a9a44ec", "class_name": "RelatedNodeInfo"}}, "text": "\",\n    #             }\n    #         ],\n    #         \"read_systems_architecture\",\n    #         record_fixtures=False,\n    #     )\n    #     self.assertEqual(message.content[0].name, 'read_system_architecture')\n    \n    # def test_read_systems_architecture_and_update_google_document(self):\n    #     message = self.get_or_record_tool_message(\n    #         SYSTEM_PROMPT,\n    #         [\n    #             {\n    #                 \"role\": \"user\",\n    #                 \"content\": \"Update the google document with details about your implementation.\",\n    #             }\n    #         ],\n    #         \"read_systems_architecture_and_update_google_document\",\n    #         record_fixtures=False,\n    #     )\n\n    #     self.assertEqual(len(message.content), 2)\n    #     print(message.content[0].input)\n    #     print(message.content[1].input)\n    #     self.assertEqual(message.content[0].name, 'read_system_architecture')\n    #     self.assertEqual(message.content[1].name, 'update_google_document')", "mimetype": "text/plain", "start_char_idx": 6071, "end_char_idx": 7072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f8889ab-fa1e-44f8-9ce0-7f4fa6ed673d": {"__data__": {"id_": "9f8889ab-fa1e-44f8-9ce0-7f4fa6ed673d", "embedding": null, "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/views.py", "file_name": "views.py", "file_type": "text/x-python", "file_size": 27, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcb9c464-7348-4762-a52d-360f6b8ff2c7", "node_type": "4", "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/views.py", "file_name": "views.py", "file_type": "text/x-python", "file_size": 27, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}, "hash": "c302868ee7079aae5abbfc66d96ed5f09bac64586dc11e92957a2d7974f5cece", "class_name": "RelatedNodeInfo"}}, "text": "file path: /Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/views.py\n              \n# Create your views here.", "mimetype": "text/plain", "start_char_idx": 15, "end_char_idx": 151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"661a96af-57fb-4e8f-9517-8238311f3a95": {"node_ids": ["58dcf6fe-bf74-4458-be44-319f9b84b36f"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/admin.py", "file_name": "admin.py", "file_type": "text/x-python", "file_size": 30, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}}, "8a8d3a34-dac1-41a4-b9ab-ee90c386dc40": {"node_ids": ["88b17e9f-956c-4eb1-92bf-e688ed25c258"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/anthropic_integration.py", "file_name": "anthropic_integration.py", "file_type": "text/x-python", "file_size": 2056, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}}, "1039f598-6f36-4424-b125-213f20fe8d6c": {"node_ids": ["a4b8923d-aff2-4030-9822-b5d83752f55d"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/apps.py", "file_name": "apps.py", "file_type": "text/x-python", "file_size": 138, "creation_date": "2024-09-25", "last_modified_date": "2024-09-25"}}, "f4356fe5-c43f-4a01-9afd-d8fcf5bc2f46": {"node_ids": ["ba081ec1-8ed1-4b30-b719-ff5a1aaf5bca", "97dadca5-260e-4a54-a561-aee4b2ca6af5", "6dd01734-3b57-4de6-bcae-b7946778a9b8"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/create_pg_vector_store.py", "file_name": "create_pg_vector_store.py", "file_type": "text/x-python", "file_size": 6652, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}}, "0671e9fd-6691-4525-bb87-0a4f1fdc4c25": {"node_ids": ["40b2f85e-6da8-4382-87e9-e8de4f7e93a2"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/llama_index.py", "file_name": "llama_index.py", "file_type": "text/x-python", "file_size": 3063, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}}, "6945afe5-733f-4116-92c0-a6c096c02792": {"node_ids": ["99115434-9e9b-46ef-9930-16838ff116eb"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/management/commands/response_types.py", "file_name": "response_types.py", "file_type": "text/x-python", "file_size": 659, "creation_date": "2024-09-30", "last_modified_date": "2024-09-30"}}, "d82ddedc-3bed-4cda-ae73-45ebf4d434ba": {"node_ids": ["1476b07b-878d-4c60-9238-084114b9b4b8"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/models.py", "file_name": "models.py", "file_type": "text/x-python", "file_size": 28, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}}, "08e3c918-13e4-4b7d-9270-065d47aa0952": {"node_ids": ["5abdcdf7-dcbc-44bf-b5c1-ec8933876042", "a5a11c5e-c81e-4878-ae01-bfb820d73b3a", "f663f254-73d6-4cb3-9fa3-a14c40b83ef4", "1d52f2a2-ac8a-445f-a0d9-32cb54159664", "58d09407-389f-431a-8771-40e93ad5a18d"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/respond.py", "file_name": "respond.py", "file_type": "text/x-python", "file_size": 13272, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}}, "099e13d3-cbfa-4d5d-9f87-bffa035caf3d": {"node_ids": ["12195939-c697-4e4d-9378-7227ab0bb18c"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/response_types.py", "file_name": "response_types.py", "file_type": "text/x-python", "file_size": 1350, "creation_date": "2024-10-04", "last_modified_date": "2024-10-04"}}, "11d1efd9-d86a-4499-86ad-42e0fc3a0f86": {"node_ids": ["db196e8c-cf3d-47a6-89ad-8fc62cf4eb46", "a56af8e8-55a2-49c7-bb9b-333e38478a74", "f02b6748-107b-4f96-8916-5934ce18129d"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/tests/test_anthropic_integration.py", "file_name": "test_anthropic_integration.py", "file_type": "text/x-python", "file_size": 6921, "creation_date": "2024-10-03", "last_modified_date": "2024-10-03"}}, "fcb9c464-7348-4762-a52d-360f6b8ff2c7": {"node_ids": ["9f8889ab-fa1e-44f8-9ce0-7f4fa6ed673d"], "metadata": {"file_path": "/Users/jtorreggiani/organizer/work/integrativernd/research/ai-workbench/llm/views.py", "file_name": "views.py", "file_type": "text/x-python", "file_size": 27, "creation_date": "2024-09-28", "last_modified_date": "2024-09-28"}}}}